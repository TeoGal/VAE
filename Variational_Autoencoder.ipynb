{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prTKL3d2kGZE"
      },
      "source": [
        "# Variational Autoencoder\n",
        "\n",
        "Build a basic Variational Autoencoder.\n",
        "\n",
        "Train it on the following dataset: https://github.com/bchao1/Anime-Face-Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qxq9uZAk3Lh"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MooRFGEeI1zb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import random\n",
        "from IPython import display\n",
        "\n",
        "from keras.layers import Conv2D as conv\n",
        "from keras.layers import Conv2DTranspose as convt\n",
        "from keras.layers import BatchNormalization as normalization\n",
        "from keras.layers import Flatten, Dense, Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL9rq-0uk7nS"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjhN6GgfmUfx"
      },
      "outputs": [],
      "source": [
        "# set a random seed\n",
        "np.random.seed(50)\n",
        "\n",
        "# parameters for building the model and training\n",
        "BATCH_SIZE=2000\n",
        "LATENT_DIM=512\n",
        "IMAGE_SIZE=64\n",
        "\n",
        "# Shape of input images\n",
        "INPUT_SHAPE = (IMAGE_SIZE,IMAGE_SIZE,3,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXTdjxmolDBo"
      },
      "source": [
        "## Download the Dataset\n",
        "\n",
        "You will download the Anime Faces dataset and save it to a local directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxKW6Q88KHcL"
      },
      "outputs": [],
      "source": [
        "# make the data directory\n",
        "try:\n",
        "  os.mkdir('/tmp/anime')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# download the zipped dataset to the data directory\n",
        "data_url = \"https://storage.googleapis.com/learning-datasets/Resources/anime-faces.zip\"\n",
        "data_file_name = \"animefaces.zip\"\n",
        "download_dir = '/tmp/anime/'\n",
        "urllib.request.urlretrieve(data_url, data_file_name)\n",
        "\n",
        "# extract the zip file\n",
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(download_dir)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD6WCIlclWaA"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTlx97U_JDPB"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "\n",
        "def get_dataset_slice_paths(image_dir):\n",
        "  #returns a list of paths to the image files\n",
        "  image_file_list = os.listdir(image_dir)\n",
        "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "\n",
        "  return image_paths\n",
        "\n",
        "\n",
        "def map_image(image_filename):\n",
        "  #preprocesses the images\n",
        "  img_raw = tf.io.read_file(image_filename)\n",
        "  image = tf.image.decode_jpeg(img_raw)\n",
        "\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "  image = image / 255.0\n",
        "  image = tf.reshape(image, shape=INPUT_SHAPE)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoCJ6DPJHL8"
      },
      "outputs": [],
      "source": [
        "# get the list containing the image paths\n",
        "paths = get_dataset_slice_paths(\"/tmp/anime/images/\")\n",
        "\n",
        "# shuffle the paths\n",
        "random.shuffle(paths)\n",
        "\n",
        "# split the paths list into to training (80%)\n",
        "paths_len = len(paths)\n",
        "train_paths_len = int(paths_len * 0.8)\n",
        "\n",
        "train_paths = paths[:train_paths_len]\n",
        "\n",
        "# load the training image paths into tensors, create batches and shuffle\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n",
        "training_dataset = training_dataset.map(map_image)\n",
        "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
        "\n",
        "print(f'number of batches in the training set: {len(training_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ZRga9vlonx"
      },
      "source": [
        "## Display Utilities\n",
        "\n",
        "We've also provided some utilities to help in visualizing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jC1cpLViJLIu"
      },
      "outputs": [],
      "source": [
        "# Plots a sample in a grid\n",
        "def display_faces(dataset, size=9):\n",
        "  dataset = dataset.unbatch().take(size)\n",
        "  n_cols = 3\n",
        "  n_rows = size//n_cols + 1\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  i = 0\n",
        "  for image in dataset:\n",
        "    i += 1\n",
        "    disp_img = np.reshape(image, (64,64,3))\n",
        "    plt.subplot(n_rows, n_cols, i)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(disp_img)\n",
        "\n",
        "# Displays a row of images\n",
        "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
        "  for idx, image in enumerate(disp_images):\n",
        "    plt.subplot(3, 10, offset + idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    image = np.reshape(image, shape)\n",
        "    plt.imshow(image)\n",
        "\n",
        "# Displays input and predicted images\n",
        "def display_results(disp_input_images, disp_predicted):\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "\n",
        "# Helper function to plot 16 images at a time\n",
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "  predictions = model.predict(test_input)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      img = predictions[i, :, :, :] * 255\n",
        "      img = img.astype('int32')\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eZsrZtqJOzv"
      },
      "outputs": [],
      "source": [
        "display_faces(training_dataset, size=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSBtdCVim9aC"
      },
      "source": [
        "## VAE Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHNxIUUS9ng9"
      },
      "source": [
        "### Sampling Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-3qk6ZBm0Fl"
      },
      "outputs": [],
      "source": [
        "# Sampling class, adds a Gaussian noise to the output\n",
        "# of the encoder, before feeding it to the decoder\n",
        "\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    # inputs -- output tensor from the encoder\n",
        "\n",
        "    # retrieve mean and variance of the Gaussian distribuion\n",
        "    mu, sigma = inputs\n",
        "\n",
        "    # retrieve batch size and dimension of noise\n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "\n",
        "    # generate random noise and combine it with inputs\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch,dim))\n",
        "    z = mu + tf.exp(0.5 * sigma) * epsilon\n",
        "\n",
        "    # return tensor with random noise combined\n",
        "    return  z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZjCSa7Y-Gvk"
      },
      "source": [
        "### ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VSVYjDim4Dk"
      },
      "outputs": [],
      "source": [
        "# ENCODER layers\n",
        "\n",
        "def encoder_layers(inputs, latent_dim):\n",
        "  \"\"\"\n",
        "    inputs -- batch from the dataset\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "  \"\"\"\n",
        "\n",
        "  # First conv layer\n",
        "  x = conv(32, 3, 2, 'same', activation='relu')(inputs)\n",
        "  x = normalization()(x)\n",
        "\n",
        "  # Second conv layer\n",
        "  x = conv(64, 3, 2, 'same', activation='relu')(x)\n",
        "  x = normalization()(x)\n",
        "\n",
        "  # Third conv layer\n",
        "  x = conv(128, 3, 2, 'same', activation='relu')(x)\n",
        "  batch_3 = normalization()(x)\n",
        "\n",
        "  # Flat before feeding to dense layers\n",
        "  x = Flatten()(batch_3)\n",
        "\n",
        "  # Dense layer\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = normalization()(x)\n",
        "\n",
        "  # Branch with two dense layers to output mean and std deviation\n",
        "  mu = Dense(latent_dim)(x)\n",
        "  sigma = Dense(latent_dim)(x)\n",
        "\n",
        "  # return mean, std deviation, and shape of image before flattening\n",
        "  return mu, sigma, batch_3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8Y-wLFym60N"
      },
      "outputs": [],
      "source": [
        "# ENCODER model with sampling layer before output\n",
        "\n",
        "def encoder_model(latent_dim, input_shape):\n",
        "  \"\"\"\n",
        "    latent_dim -- dimension of the latent space\n",
        "    input_shape -- shape of the batch\n",
        "  \"\"\"\n",
        "  # declare input layer\n",
        "  inputs = Input(input_shape)\n",
        "\n",
        "  # Encoder layers\n",
        "  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim)\n",
        "\n",
        "  # Sampling layer to add noise\n",
        "  z = Sampling()((mu,sigma))\n",
        "\n",
        "  # Declare all model architecture\n",
        "  model = tf.keras.Model(inputs, outputs=[mu,sigma,z])\n",
        "\n",
        "  # print summary of the model\n",
        "  model.summary()\n",
        "\n",
        "  # return model, and shape before flattening\n",
        "  return model, conv_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ENB-6a-0R5"
      },
      "source": [
        "### DECODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlTjAzgsm9Vn"
      },
      "outputs": [],
      "source": [
        "# DECODER layers\n",
        "\n",
        "def decoder_layers(inputs, conv_shape):\n",
        "  \"\"\"\n",
        "    inputs -- output of the encoder\n",
        "    conv_shape -- shape of the tensor before flattening\n",
        "  \"\"\"\n",
        "  # retrieve shape before flattening in the encoder\n",
        "  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "\n",
        "  # Dense layer\n",
        "  x = Dense(units, activation='relu')(inputs)\n",
        "  x = normalization()(x)\n",
        "\n",
        "  # reshape to match tensor before flattening in the encoder\n",
        "  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "\n",
        "  # first conv transpose layer\n",
        "  x = convt(128, 3, 2, 'same', activation='relu')(x)\n",
        "  x = normalization()(x)\n",
        "\n",
        "  # second conv transpose layer\n",
        "  x = convt(64, 3, 2, 'same', activation='relu')(x)\n",
        "  x = normalization()(x)\n",
        "\n",
        "  # third conv transpose layer\n",
        "  x = convt(32, 3, 2, 'same', activation='relu')(x)\n",
        "  x = normalization()(x)\n",
        "\n",
        "  # fourth conv transpose layer to match desired image output\n",
        "  x = convt(3, 3, 1, 'same', activation='sigmoid')(x)\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUgTyqNFm_jR"
      },
      "outputs": [],
      "source": [
        "# DECODER model\n",
        "\n",
        "def decoder_model(latent_dim, conv_shape):\n",
        "  \"\"\"\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    conv_shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "  # declare input layer\n",
        "  inputs = Input(shape=(latent_dim, ))\n",
        "\n",
        "  # Decoder layers\n",
        "  outputs = decoder_layers(inputs, conv_shape)\n",
        "\n",
        "  # declare decoder model\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  # print model summary\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps0yuE1d_cQc"
      },
      "source": [
        "### Kullback–Leibler loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tngFmDDwnDn-"
      },
      "outputs": [],
      "source": [
        "# Implement KL-loss from https://arxiv.org/abs/2002.07514\n",
        "\n",
        "def kl_reconstruction_loss(mu, sigma):\n",
        "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "  return tf.reduce_mean(kl_loss) * -0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1I431I_og7"
      },
      "source": [
        "### VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuPHg28JnGCp"
      },
      "outputs": [],
      "source": [
        "def vae_model(encoder, decoder, input_shape):\n",
        "  \"\"\"\n",
        "    encoder -- encoder model\n",
        "    decoder -- decoder model\n",
        "    input_shape -- shape of batch\n",
        "  \"\"\"\n",
        "  # declare input layer\n",
        "  inputs = Input(shape=input_shape)\n",
        "\n",
        "  # Encoder\n",
        "  mu, sigma, z = encoder(inputs)\n",
        "\n",
        "  # Decoder\n",
        "  reconstructed = decoder(z)\n",
        "\n",
        "  # declare VAE model\n",
        "  model = tf.keras.Model(inputs, reconstructed)\n",
        "\n",
        "  # Use costumized KL-loss\n",
        "  loss = kl_reconstruction_loss(mu, sigma)\n",
        "  model.add_loss(loss)\n",
        "\n",
        "  # return VAE model\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Encoder and retireve shape before flattening\n",
        "encoder, conv_shape = encoder_model(LATENT_DIM, INPUT_SHAPE)\n",
        "\n",
        "# Initialize decoder\n",
        "decoder = decoder_model(LATENT_DIM, conv_shape)\n",
        "\n",
        "# Initialize VAE\n",
        "vae = vae_model(encoder, decoder, INPUT_SHAPE)\n"
      ],
      "metadata": {
        "id": "DeNJ2wZKEuBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6IwN5vlAb5w"
      },
      "source": [
        "## Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHPwSmZFnQ_2"
      },
      "outputs": [],
      "source": [
        "# configuration for training\n",
        "epochs = 100\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvL1bHXJnajM"
      },
      "outputs": [],
      "source": [
        "# Training loop.\n",
        "\n",
        "# random vector for generation\n",
        "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
        "\n",
        "# print initial images\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, x_batch_train in enumerate(training_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # forward prop\n",
        "      reconstructed = vae(x_batch_train)\n",
        "\n",
        "      #reconstruction loss\n",
        "      flattened_inputs = tf.reshape(x_batch_train, (-1,1))\n",
        "      flattened_outputs = tf.reshape(reconstructed, (-1,1))\n",
        "\n",
        "      loss = mse_loss(flattened_inputs, flattened_outputs)*64*64*3\n",
        "\n",
        "      loss += sum(vae.losses)\n",
        "\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "    loss_metric(loss)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      display.clear_output(wait=False)\n",
        "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
        "    print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}